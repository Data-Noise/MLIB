<!DOCTYPE html>
<html>


<link rel="stylesheet" href="../style/style.css">
<p class="mytext">Ссылки:</p>

<p style="text-align: left;"><a class="mytext" style="color: chocolate;
"href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></p> 
<p style="text-align: left;"><a class="mytext" style="color: chocolate;
"href="https://arxiv.org/pdf/2201.02177">https://arxiv.org/pdf/2201.02177</a></p> 
<p style="text-align: left;"><a class="mytext" style="color: chocolate;
"href="https://arxiv.org/pdf/2405.15071">https://arxiv.org/pdf/2405.15071</a> </p> 
<p class="mytext">


</p>



<p class="mytext" ><br>
Ссылки:
https://arxiv.org/pdf/2201.02177 -- гроккинг
https://arxiv.org/pdf/2405.15071 -- более занятная статья, в которой рассказывается о том каким образом трансформерам удаётся рассуждать, каким образом они хранят в памяти объекты и т.д.

Термины: Token, Emvedding, Positional encoding, M_k,M_v,M_q, M_o, hidden states.

Устройство архитектуры transformer 
Chapters:
	Input Embedding
	Positional Encoding
	Multi-Head Attention
	Normalization
	Feed Forward
	Синтез

Chapter 1:
Чтобы обработать текстовую информацию, текст разбивается на токены [token], т.е. на кусочки разного размера, условно токены можно описать как приставки, суффиксы, знаки препинания и т.д. которым выдаются определённые индексы в словаре. 
После токенизации каждому токену присваивается векторное представление эмбеддинг[embedding], т.е. каждый токен имеет вектор определённой размерности, обычно используются вектора размерностью в 512 элементов.
Значения эмбеддингов берутся из словаря, который хранит эмбеддинги в многомерном векторном пространстве, внутри которого они разделены на различные группы. Подобные векторные представления могут быть заранее заготовленными и статичными [word2vec] или в процессе обучения модели они могут формироваться с нуля [sentence transformers] и корректироваться в процессе.
Для того, чтобы модель могла учитывать порядок слов используется positionalencoding. Каждому вектору присваиваются небольшие изменения, формулы для этого могут отличаться, так как задача довольно простая, просто избежать появления одинаковых эмбеддингов. 

Chapter 2:
Затем матрицу из эмбеддингов отправляют в модуль Multi-HeadAttention. Multi в названии означает, что этот модуль состоит из какого-то числа блоков Self-Attention. Матрицу эмбеддингов умножают на весовые матрицы $M_k^n, M_v^n, M_q^n$, где n означает номер self-attention, а индексы означают названия матриц, которые получаются в результате умножения (матрицы Q,K,V).
[Нарисовать картинку]
Стоит отметить, что после умножений эмбеддингов на весовые матрицы у матриц Q, K, V должна быть кратно снижена размерность относительно числа блоков self-attention, так как в дальнейшем результаты со всех self-attention будут подвергнуты конкатенации, тем сравнявшись по размерности со входным эмбеддингом. Т.е. если блоков self-attention 2, то матрицы Q,K,V должны иметь размерность 256×n, а если 4, то 128×n и т.д.
[Нарисовать картинку]
Стоит немного поговорить о том, какой смысл скрывается за матричными умножениями. Для этого стоит немного отойти от темы и посмотреть на идею матриц. Матрицы способны не только хранить переменные, описывать базисы, и векторы, т.п. они ещё могут служить фильтрами и мини поисковыми системами. 
Привести пример с картинками

Блоки self-attention посредством градиентного спуска обучаются обращать внимание на конкретные фрагменты текста, т. е. Один блок обращает внимание на запятые, 2-й блок обращает внимание на кавыки, другой блок на глаголы и т. д.  

Затем эмбеддинги из блоков Self-Attention подвергаются конкатенации, собираясь в матрицу равной по размерности входному эмбеддингу. Затем эта матрица умножается на матрицу W_o, это делается для того, чтобы смешать признаки. 

Т.е. каждый эмбеддинг слова сохраняет внутри себя весь контекст предложения. Отсюда следует, что предсказание нового слова становится довольно простой задачей, достаточно отправить на вход FFN последний эмбеддинг. Сеть предсказывает новый эмбеддинг, который добавляется к прошлым эмбеддингам. Затем цикл повторяется снова, эмбеддинги проходят через SA, обновляя свой контекст, а последнее слово сохраняет весь прошлый контекст в себе, тем самым позволяя модели предсказать следующее слово. 

А вот каким образом FFN хранит факты кажется весьма интересным вопросом. В статьях это объясняется тем, что модель аппроксимировала марковские цепи очень высоких порядков. Что кажется довольно разумным, можно сказать, что это марковские цепи, но более экономно расходующие пространство.

Тем не менее, это не даёт ответа на то каким образом сеть аппроксимирует модель мира. И не слишком объясняет способности к рассуждениям. Конечно, марковские цепи вполне могут аппроксимировать и функции, но я не понимаю, как работает граф знаний внутри этой сети. Каким образом модель составляет высокоуровневые признаки\понятия из более низкоуровневых. 


Каким образом эта матрца работает я не совсем понимании (поищи информацию на ютубе)
1) каким образом признаки записываются в эмбеддинги и как потом смешиваются.
2) матрица проекции W_0

Затем эмбеддинги проходят нормализацию и поочерёдно попадают на вход FFN, которая, предсказывает на входе выходной эмбеддинг для каждого входного эмбеддинга. 

</p>



</html>